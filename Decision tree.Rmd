---
title: "Decision Tree"
author: "HAIXIA BIAN"
date: "3/25/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tree)
library(caret)
library(rpart)
library("rpart.plot")
library(rattle)
library(RColorBrewer)
library(ROCR)
library(cvAUC)
```

```{r}
mydata = read.csv(file.choose())
head(mydata)
attach(mydata)
```

```{r}
#Transfer stageID to be categorical data
mydata$stageId <- as.factor(mydata$stageId)
cleandata <- mydata[,-c(0,1)]
head(cleandata)

#Split training and test set
#train_data <- cleandata[c(1:(0.7*nrow(cleandata))),]
#test_data <- cleandata[-c(1:(0.7*nrow(cleandata))),]
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data <- train_data[,-c(18,19)]
test_data <- test_data[,-c(18,19)]
train_data$stageId <- as.factor(train_data$stageId)
test_data$stageId <- as.factor(test_data$stageId)
```

```{r}
#Perform ten-fold cross-validation
#Defining training control
set.seed(111)
train.control <- trainControl(method = "cv", number = 10)
#train the model
model <- train(winner ~., data = train_data,
               method = "ctree2",
               trControl = train.control,
               na.action = na.exclude)
#Summarize the results
print(model)
```

```{r}
#Best model 
finalModel <- model$finalModel
#visulaize final model:
plot(finalModel, main="Final selected model")
```

```{r}
#Make prediction
test_data <- na.omit(test_data)
pred_cv <- predict(model, test_data)
confusionMatrix(pred_cv, test_data$winner)
```

```{r}
#use rpart library to Tune the hyper-parameters
model_test1 <- rpart(winner ~., data = train_data,
                     parms=list(split="gini"),
                     cp=0.0001, minsplit=2000,
                     minbucket=2000, maxdepth = 10)
rpart.plot(model_test1,cex=0.6)
#plot(model_test1); text(model_test1, pretty = 0)
#fancyRpartPlot(model_test1, cex = 0.6)
#Validation of decision tree using the ‘Complexity Parameter’ and cross validated error
printcp(model_test1)

#select least(optimal) cp to make cv error rate is minimum. 
model_test1$cptable[which.min(model_test1$cptable[,"xerror"]), "CP"] 

loccopy <- function(n, digits = 3){
  data <- locator(n)
  data <- round(cbind(data$x, data$y), digits)
  clip <- pipe("pbcopy", "W")
  write.table(data, file = clip, col.names = F, row.names = F)
  close(clip)
}


plotcp(model_test1, minline = TRUE, lty = 3, col = 1, upper = "none",
       main = "Complexity Parameter Table for an Rpart Fit") #The error nearly remains unchanged after cp<0.001, so we choose cp=0.001
#loccopy(1)
?plotcp
```

```{r}
selected_model <- rpart(winner ~., data = train_data,
                        parms=list(split="gini"),
                        cp=0.001, minsplit=2000,
                        minbucket=2000, maxdepth = 10)
printcp(selected_model)
rpart.plot(selected_model,cex=0.6)

#Prune the tree to create an optimal decision tree
ptree <- prune(selected_model,
               cp=selected_model$cptable[which.min(selected_model$cptable[,"xerror"]), "CP"])

rpart.plot(ptree, uniform=TRUE,
           main="Pruned Classification Tree",
           cex = 0.6)

fancyRpartPlot(ptree, uniform=TRUE,
               main="Pruned Classification Tree",
               cex = 0.8)
```

```{r}
tree.data <- tree(winner~.,train_data)
summary(tree.data)
plot(tree.data); text(tree.data ,pretty =0)
```

```{r}
#Make a prediction 
pred <- predict(selected_model, test_data, type = 'class')

#Test on test set
confusionMatrix(pred, test_data$winner)
```

```{r}
#measurement
#ROC curve
pred_prob <- predict(selected_model, test_data, type = 'prob')
#write.csv(pred_prob,'DecisionTree_pred.csv')
prediction <- prediction(pred_prob[,2], test_data$winner)
perf <- performance(prediction, "tpr", "fpr")
DecisionTree_AUC <- AUC(pred_prob[,2], test_data$winner)
l = sprintf("Decision tree AUC: %f", DecisionTree_AUC)
plot(perf, colorize=TRUE,
     main="ROC Curve"); legend("topleft", legend = l, cex=0.6)

```




